{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0f9fc781dd77605c91502f83c2253d61998bc4244436d43aafe4ad527d617ce42",
   "display_name": "Python 3.8.5 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Homework\n",
    "\n",
    "Generated tasks from task_01:  \n",
    "\n",
    "| parameter | value |\n",
    "| --------- | :-----: |\n",
    "| Type | Regression |\n",
    "| Data set | Boston Housing Data Set |\n",
    "| Method 1 | Linear regression |\n",
    "| Method 2 | Nadaraya-Watson |\n",
    "\n",
    "python 3.8.5 64-bit (conda)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data imports\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Data normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Used backend libraries\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# For tests and representation of results\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n"
   ]
  },
  {
   "source": [
    "## Data loading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X shape: (506, 13)\nY shape: (506,)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "0       15.3  396.90   4.98  \n",
       "1       17.8  396.90   9.14  \n",
       "2       17.8  392.83   4.03  \n",
       "3       18.7  394.63   2.94  \n",
       "4       18.7  396.90   5.33  \n",
       "..       ...     ...    ...  \n",
       "501     21.0  391.99   9.67  \n",
       "502     21.0  396.90   9.08  \n",
       "503     21.0  396.90   5.64  \n",
       "504     21.0  393.45   6.48  \n",
       "505     21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0.0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1.0</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.06263</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0.0</td>\n      <td>0.573</td>\n      <td>6.593</td>\n      <td>69.1</td>\n      <td>2.4786</td>\n      <td>1.0</td>\n      <td>273.0</td>\n      <td>21.0</td>\n      <td>391.99</td>\n      <td>9.67</td>\n    </tr>\n    <tr>\n      <th>502</th>\n      <td>0.04527</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0.0</td>\n      <td>0.573</td>\n      <td>6.120</td>\n      <td>76.7</td>\n      <td>2.2875</td>\n      <td>1.0</td>\n      <td>273.0</td>\n      <td>21.0</td>\n      <td>396.90</td>\n      <td>9.08</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>0.06076</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0.0</td>\n      <td>0.573</td>\n      <td>6.976</td>\n      <td>91.0</td>\n      <td>2.1675</td>\n      <td>1.0</td>\n      <td>273.0</td>\n      <td>21.0</td>\n      <td>396.90</td>\n      <td>5.64</td>\n    </tr>\n    <tr>\n      <th>504</th>\n      <td>0.10959</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0.0</td>\n      <td>0.573</td>\n      <td>6.794</td>\n      <td>89.3</td>\n      <td>2.3889</td>\n      <td>1.0</td>\n      <td>273.0</td>\n      <td>21.0</td>\n      <td>393.45</td>\n      <td>6.48</td>\n    </tr>\n    <tr>\n      <th>505</th>\n      <td>0.04741</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0.0</td>\n      <td>0.573</td>\n      <td>6.030</td>\n      <td>80.8</td>\n      <td>2.5050</td>\n      <td>1.0</td>\n      <td>273.0</td>\n      <td>21.0</td>\n      <td>396.90</td>\n      <td>7.88</td>\n    </tr>\n  </tbody>\n</table>\n<p>506 rows × 13 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# Load dataset\n",
    "boston_dataset = load_boston()\n",
    "# Variables for learning\n",
    "X_raw = boston_dataset.data\n",
    "Y_raw = boston_dataset.target\n",
    "# Show data\n",
    "print(f\"X shape: {X_raw.shape}\")\n",
    "print(f\"Y shape: {Y_raw.shape}\")\n",
    "pd.DataFrame(X_raw, columns=boston_dataset.feature_names)"
   ]
  },
  {
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Is used minmax normalization for data:\n",
    "\n",
    "$$\n",
    "x_{norm} = \\frac{x - \\displaystyle\\min_{x \\in X}(x)}{\\displaystyle\\max_{x \\in X}(x) - \\displaystyle\\min_{x \\in X}(x)}\n",
    "$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         CRIM    ZN     INDUS  CHAS       NOX        RM       AGE       DIS  \\\n",
       "0    0.000000  0.18  0.067815   0.0  0.314815  0.577505  0.641607  0.269203   \n",
       "1    0.000236  0.00  0.242302   0.0  0.172840  0.547998  0.782698  0.348962   \n",
       "2    0.000236  0.00  0.242302   0.0  0.172840  0.694386  0.599382  0.348962   \n",
       "3    0.000293  0.00  0.063050   0.0  0.150206  0.658555  0.441813  0.448545   \n",
       "4    0.000705  0.00  0.063050   0.0  0.150206  0.687105  0.528321  0.448545   \n",
       "..        ...   ...       ...   ...       ...       ...       ...       ...   \n",
       "501  0.000633  0.00  0.420455   0.0  0.386831  0.580954  0.681771  0.122671   \n",
       "502  0.000438  0.00  0.420455   0.0  0.386831  0.490324  0.760041  0.105293   \n",
       "503  0.000612  0.00  0.420455   0.0  0.386831  0.654340  0.907312  0.094381   \n",
       "504  0.001161  0.00  0.420455   0.0  0.386831  0.619467  0.889804  0.114514   \n",
       "505  0.000462  0.00  0.420455   0.0  0.386831  0.473079  0.802266  0.125072   \n",
       "\n",
       "          RAD       TAX   PTRATIO         B     LSTAT  \n",
       "0    0.000000  0.208015  0.287234  1.000000  0.089680  \n",
       "1    0.043478  0.104962  0.553191  1.000000  0.204470  \n",
       "2    0.043478  0.104962  0.553191  0.989737  0.063466  \n",
       "3    0.086957  0.066794  0.648936  0.994276  0.033389  \n",
       "4    0.086957  0.066794  0.648936  1.000000  0.099338  \n",
       "..        ...       ...       ...       ...       ...  \n",
       "501  0.000000  0.164122  0.893617  0.987619  0.219095  \n",
       "502  0.000000  0.164122  0.893617  1.000000  0.202815  \n",
       "503  0.000000  0.164122  0.893617  1.000000  0.107892  \n",
       "504  0.000000  0.164122  0.893617  0.991301  0.131071  \n",
       "505  0.000000  0.164122  0.893617  1.000000  0.169702  \n",
       "\n",
       "[506 rows x 13 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.18</td>\n      <td>0.067815</td>\n      <td>0.0</td>\n      <td>0.314815</td>\n      <td>0.577505</td>\n      <td>0.641607</td>\n      <td>0.269203</td>\n      <td>0.000000</td>\n      <td>0.208015</td>\n      <td>0.287234</td>\n      <td>1.000000</td>\n      <td>0.089680</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000236</td>\n      <td>0.00</td>\n      <td>0.242302</td>\n      <td>0.0</td>\n      <td>0.172840</td>\n      <td>0.547998</td>\n      <td>0.782698</td>\n      <td>0.348962</td>\n      <td>0.043478</td>\n      <td>0.104962</td>\n      <td>0.553191</td>\n      <td>1.000000</td>\n      <td>0.204470</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000236</td>\n      <td>0.00</td>\n      <td>0.242302</td>\n      <td>0.0</td>\n      <td>0.172840</td>\n      <td>0.694386</td>\n      <td>0.599382</td>\n      <td>0.348962</td>\n      <td>0.043478</td>\n      <td>0.104962</td>\n      <td>0.553191</td>\n      <td>0.989737</td>\n      <td>0.063466</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000293</td>\n      <td>0.00</td>\n      <td>0.063050</td>\n      <td>0.0</td>\n      <td>0.150206</td>\n      <td>0.658555</td>\n      <td>0.441813</td>\n      <td>0.448545</td>\n      <td>0.086957</td>\n      <td>0.066794</td>\n      <td>0.648936</td>\n      <td>0.994276</td>\n      <td>0.033389</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000705</td>\n      <td>0.00</td>\n      <td>0.063050</td>\n      <td>0.0</td>\n      <td>0.150206</td>\n      <td>0.687105</td>\n      <td>0.528321</td>\n      <td>0.448545</td>\n      <td>0.086957</td>\n      <td>0.066794</td>\n      <td>0.648936</td>\n      <td>1.000000</td>\n      <td>0.099338</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.000633</td>\n      <td>0.00</td>\n      <td>0.420455</td>\n      <td>0.0</td>\n      <td>0.386831</td>\n      <td>0.580954</td>\n      <td>0.681771</td>\n      <td>0.122671</td>\n      <td>0.000000</td>\n      <td>0.164122</td>\n      <td>0.893617</td>\n      <td>0.987619</td>\n      <td>0.219095</td>\n    </tr>\n    <tr>\n      <th>502</th>\n      <td>0.000438</td>\n      <td>0.00</td>\n      <td>0.420455</td>\n      <td>0.0</td>\n      <td>0.386831</td>\n      <td>0.490324</td>\n      <td>0.760041</td>\n      <td>0.105293</td>\n      <td>0.000000</td>\n      <td>0.164122</td>\n      <td>0.893617</td>\n      <td>1.000000</td>\n      <td>0.202815</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>0.000612</td>\n      <td>0.00</td>\n      <td>0.420455</td>\n      <td>0.0</td>\n      <td>0.386831</td>\n      <td>0.654340</td>\n      <td>0.907312</td>\n      <td>0.094381</td>\n      <td>0.000000</td>\n      <td>0.164122</td>\n      <td>0.893617</td>\n      <td>1.000000</td>\n      <td>0.107892</td>\n    </tr>\n    <tr>\n      <th>504</th>\n      <td>0.001161</td>\n      <td>0.00</td>\n      <td>0.420455</td>\n      <td>0.0</td>\n      <td>0.386831</td>\n      <td>0.619467</td>\n      <td>0.889804</td>\n      <td>0.114514</td>\n      <td>0.000000</td>\n      <td>0.164122</td>\n      <td>0.893617</td>\n      <td>0.991301</td>\n      <td>0.131071</td>\n    </tr>\n    <tr>\n      <th>505</th>\n      <td>0.000462</td>\n      <td>0.00</td>\n      <td>0.420455</td>\n      <td>0.0</td>\n      <td>0.386831</td>\n      <td>0.473079</td>\n      <td>0.802266</td>\n      <td>0.125072</td>\n      <td>0.000000</td>\n      <td>0.164122</td>\n      <td>0.893617</td>\n      <td>1.000000</td>\n      <td>0.169702</td>\n    </tr>\n  </tbody>\n</table>\n<p>506 rows × 13 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "X_norm = scaler.fit_transform(X_raw)\n",
    "# Show results\n",
    "pd.DataFrame(X_norm, columns=boston_dataset.feature_names)"
   ]
  },
  {
   "source": [
    "## Linear Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "\n",
    "    def __init__(self, rate: float = 0.2, steps_count: int = 100):\n",
    "        self.rate_ = rate\n",
    "        self.n_steps_ = steps_count\n",
    "        self.weights_ = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(X, Y, weights):\n",
    "        \"\"\"\n",
    "        Compute gradient for linear regression.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array_like\n",
    "            Input dataset of shape (m, n + 1)\n",
    "        Y: array_like\n",
    "            Array of real values of shape (m, )\n",
    "        weights: array_like\n",
    "            Weights of shape (n + 1, ) for linear regression learning\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        grad_cost: array_like\n",
    "            Computed gradient for input weights and data\n",
    "        \"\"\"\n",
    "        return (2 / X.shape[0]) * np.dot(X.T, (np.dot(X, weights) - Y))\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Fit linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array_like\n",
    "            Input dataset of shape (m, n)\n",
    "        Y: array_like\n",
    "            Array of real values of shape (m, )\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        # Add bias term for input data\n",
    "        X_train = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        # Started weights initialization \n",
    "        self.weights_ = np.zeros(X_train.shape[1])\n",
    "\n",
    "        # Learning loop\n",
    "        for _ in range(self.n_steps_):\n",
    "            self.weights_ = self.weights_ - self.rate_ * LinearRegression.gradient(X_train, Y, self.weights_)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict using the linear regression.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: array_like\n",
    "            Samples of shape (samples_count, m)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred_values: array_like\n",
    "            Predicted values of shape (samples_count, )\n",
    "        \"\"\"\n",
    "        return np.dot(np.hstack((np.ones((x.shape[0], 1)), x)), self.weights_)"
   ]
  },
  {
   "source": [
    "## Nadaraya-Watson method\n",
    "\n",
    "$$\n",
    "K(d) - kernel,\\ h(x, X) - Parzen\\ window\\ width\\ (static\\ or\\ dynamic) \\\\\n",
    "a(x, X^l)= \\frac{\\sum_{i=1}^{l} y_i K(\\frac{\\rho(x, x_i)}{h(x, X)})}{\\sum_{i=1}^{l} K(\\frac{\\rho(x, x_i)}{h(x, X)})}\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Kernels\n",
    "#### Epanechnikov kernel\n",
    "$$\n",
    "E(d) = \\frac{3}{4} (1 - d^2)\\cdot\\mathbb{1}\\{|d| \\leq 1\\}\n",
    "$$\n",
    "####  Gaussian kernel\n",
    "$$\n",
    "G(d) = \\frac{1}{\\sqrt{2 \\pi}} e^{\\frac{-d^2}{2}}\n",
    "$$\n",
    "#### Tophat (Uniform) kernel\n",
    "$$\n",
    "P(d) = \\frac{1}{2}\\cdot\\mathbb{1}\\{|d| \\leq 1\\}\n",
    "$$\n",
    "#### Triangular kernel\n",
    "$$\n",
    "T(d) = (1 - |d|)\\cdot\\mathbb{1}\\{|d| \\leq 1\\}\n",
    "$$\n",
    "#### Quartic kernel\n",
    "$$\n",
    "Q(d) = \\frac{15}{16}(1 - d^2)^2\\cdot\\mathbb{1}\\{|d| \\leq 1\\}\n",
    "$$\n",
    "#### Sigmoid kernel\n",
    "$$\n",
    "S(d) = \\frac{2}{\\pi}\\frac{1}{e^d + e^{-d}}\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epanechnikov kernel\n",
    "def E(distance, window_width):\n",
    "    dist = np.array(distance) / window_width\n",
    "    return 0.75 * (1 - dist ** 2) * (np.abs(dist) <= 1)\n",
    "\n",
    "# Gaussian kernel\n",
    "def G(distance, window_width):\n",
    "    dist = np.array(distance) / window_width\n",
    "    return (1 / math.sqrt(2 * math.pi)) * np.exp((-(dist ** 2)) / 2)\n",
    "\n",
    "# Tophat (Uniform) kernel\n",
    "def P(distance, window_width):\n",
    "    dist = np.array(distance) / window_width\n",
    "    return 0.5 * (np.abs(dist) <= 1)\n",
    "\n",
    "# Triangular kernel\n",
    "def T(distance, window_width):\n",
    "    dist = np.array(distance) / window_width\n",
    "    return (1 - np.abs(dist)) * (np.abs(dist) <= 1)\n",
    "\n",
    "# Quartic kernel\n",
    "def Q(distance, window_width):\n",
    "    dist = np.array(distance) / window_width\n",
    "    return (15.0 / 16.0) * ((1 - dist ** 2) ** 2) * (np.abs(dist) <= 1)\n",
    "\n",
    "# Sigmoid kerne\n",
    "def S(distance, window_width):\n",
    "    dist = np.array(distance) / window_width\n",
    "    return (2 / math.pi) / (np.exp(dist) + np.exp(-dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NadarayaWatson(object):\n",
    "\n",
    "    def __init__(self, kernel=E, k: int = 3):\n",
    "        self.X_ = None\n",
    "        self.Y_ = None\n",
    "        self.kernel_ = kernel\n",
    "        self.parzen_window_ = None\n",
    "        self.k_ = k\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Fit Nadaraya-Watson model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array_like\n",
    "            Input dataset of shape (m, n)\n",
    "        Y: array_like\n",
    "            Array of real values of shape (m, )\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # For Nadaraya-Watson simple is saved samples\n",
    "        self.X_ = np.array(X)\n",
    "        self.Y_ = np.array(Y)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict using the Nadaraya-Watson method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: array_like\n",
    "            Samples of shape (samples_count, m)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred_values: array_like\n",
    "            Predicted values of shape (samples_count, )\n",
    "        \"\"\"\n",
    "        distances = cdist(x, self.X_)\n",
    "        # Use dynamic parzen window (k+1 neighbor)\n",
    "        h = np.sort(distances)[:,self.k_]\n",
    "        kernel_res = self.kernel_(distances, h[:, None])\n",
    "        features_sum = np.sum(self.Y_ * kernel_res, axis=-1)\n",
    "        # Add small constant to total sum to avoid float arithmetic warning\n",
    "        return features_sum / (np.sum(kernel_res, axis=-1) + 1e-10)"
   ]
  },
  {
   "source": [
    "## Tests and Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_kfold = 10\n",
    "kf = KFold(num_kfold)\n",
    "\n",
    "def test(Model, parameters_1, parameters_2, X, Y):\n",
    "    results = []\n",
    "    for parameter_1 in parameters_1:\n",
    "        results.append([])\n",
    "        for parameter_2 in parameters_2:\n",
    "            rmse_sum = 0.0\n",
    "            # Split on test and train samples\n",
    "            for train_indices, test_indices in kf.split(X):\n",
    "                model = Model(parameter_1, parameter_2)\n",
    "                model.fit(X[train_indices], Y[train_indices])\n",
    "                Y_pred = model.predict(X[test_indices])\n",
    "                # Calculate rmse\n",
    "                rmse_sum += mean_squared_error(Y[test_indices], Y_pred, squared=False)\n",
    "            # Add avarage rmse to results\n",
    "            results[-1].append(rmse_sum / num_kfold)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            20         40         60         80         100        200   \\\n",
       "0.001  20.778687  18.417748  16.453137  14.844076  13.558158  10.474570   \n",
       "0.005  13.473830  10.441850   9.748350   9.463677   9.235450   8.260907   \n",
       "0.010  10.402002   9.465960   9.017748   8.608273   8.258991   7.254525   \n",
       "0.020   9.471408   8.605775   7.967131   7.540426   7.252063   6.615603   \n",
       "0.030   9.020984   7.962974   7.379769   7.049144   6.842963   6.287378   \n",
       "0.040   8.599907   7.533654   7.047483   6.788878   6.615176   6.019703   \n",
       "0.050   8.242432   7.244693   6.841317   6.614969   6.440773   5.797988   \n",
       "0.070   7.713019   6.898579   6.577334   6.346587   6.146858   5.467390   \n",
       "0.100   7.232476   6.613998   6.286552   6.018389   5.796694   5.162284   \n",
       "0.200   6.612375   6.016148   5.612526   5.342773   5.160531   4.815270   \n",
       "\n",
       "           400       800       1500      2000      3000  \n",
       "0.001  9.462069  8.610280  7.638435  7.256743  6.845403  \n",
       "0.005  7.255757  6.615934  6.082746  5.799144  5.405022  \n",
       "0.010  6.615823  6.020351  5.404906  5.163860  4.917551  \n",
       "0.020  6.020136  5.346741  4.917459  4.816111  4.762238  \n",
       "0.030  5.616942  5.037903  4.790733  4.762228  4.787130  \n",
       "0.040  5.346301  4.889386  4.762217  4.773119  4.840584  \n",
       "0.050  5.163160  4.815971  4.767667  4.803799  4.895988  \n",
       "0.070  4.950216  4.765076  4.812734  4.877968  4.988251  \n",
       "0.100  4.815737  4.773129  4.896044  4.974975  5.075905  \n",
       "0.200  4.773145  4.913540  5.075985  5.126820  5.165297  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>20</th>\n      <th>40</th>\n      <th>60</th>\n      <th>80</th>\n      <th>100</th>\n      <th>200</th>\n      <th>400</th>\n      <th>800</th>\n      <th>1500</th>\n      <th>2000</th>\n      <th>3000</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0.001</th>\n      <td>20.778687</td>\n      <td>18.417748</td>\n      <td>16.453137</td>\n      <td>14.844076</td>\n      <td>13.558158</td>\n      <td>10.474570</td>\n      <td>9.462069</td>\n      <td>8.610280</td>\n      <td>7.638435</td>\n      <td>7.256743</td>\n      <td>6.845403</td>\n    </tr>\n    <tr>\n      <th>0.005</th>\n      <td>13.473830</td>\n      <td>10.441850</td>\n      <td>9.748350</td>\n      <td>9.463677</td>\n      <td>9.235450</td>\n      <td>8.260907</td>\n      <td>7.255757</td>\n      <td>6.615934</td>\n      <td>6.082746</td>\n      <td>5.799144</td>\n      <td>5.405022</td>\n    </tr>\n    <tr>\n      <th>0.010</th>\n      <td>10.402002</td>\n      <td>9.465960</td>\n      <td>9.017748</td>\n      <td>8.608273</td>\n      <td>8.258991</td>\n      <td>7.254525</td>\n      <td>6.615823</td>\n      <td>6.020351</td>\n      <td>5.404906</td>\n      <td>5.163860</td>\n      <td>4.917551</td>\n    </tr>\n    <tr>\n      <th>0.020</th>\n      <td>9.471408</td>\n      <td>8.605775</td>\n      <td>7.967131</td>\n      <td>7.540426</td>\n      <td>7.252063</td>\n      <td>6.615603</td>\n      <td>6.020136</td>\n      <td>5.346741</td>\n      <td>4.917459</td>\n      <td>4.816111</td>\n      <td>4.762238</td>\n    </tr>\n    <tr>\n      <th>0.030</th>\n      <td>9.020984</td>\n      <td>7.962974</td>\n      <td>7.379769</td>\n      <td>7.049144</td>\n      <td>6.842963</td>\n      <td>6.287378</td>\n      <td>5.616942</td>\n      <td>5.037903</td>\n      <td>4.790733</td>\n      <td>4.762228</td>\n      <td>4.787130</td>\n    </tr>\n    <tr>\n      <th>0.040</th>\n      <td>8.599907</td>\n      <td>7.533654</td>\n      <td>7.047483</td>\n      <td>6.788878</td>\n      <td>6.615176</td>\n      <td>6.019703</td>\n      <td>5.346301</td>\n      <td>4.889386</td>\n      <td>4.762217</td>\n      <td>4.773119</td>\n      <td>4.840584</td>\n    </tr>\n    <tr>\n      <th>0.050</th>\n      <td>8.242432</td>\n      <td>7.244693</td>\n      <td>6.841317</td>\n      <td>6.614969</td>\n      <td>6.440773</td>\n      <td>5.797988</td>\n      <td>5.163160</td>\n      <td>4.815971</td>\n      <td>4.767667</td>\n      <td>4.803799</td>\n      <td>4.895988</td>\n    </tr>\n    <tr>\n      <th>0.070</th>\n      <td>7.713019</td>\n      <td>6.898579</td>\n      <td>6.577334</td>\n      <td>6.346587</td>\n      <td>6.146858</td>\n      <td>5.467390</td>\n      <td>4.950216</td>\n      <td>4.765076</td>\n      <td>4.812734</td>\n      <td>4.877968</td>\n      <td>4.988251</td>\n    </tr>\n    <tr>\n      <th>0.100</th>\n      <td>7.232476</td>\n      <td>6.613998</td>\n      <td>6.286552</td>\n      <td>6.018389</td>\n      <td>5.796694</td>\n      <td>5.162284</td>\n      <td>4.815737</td>\n      <td>4.773129</td>\n      <td>4.896044</td>\n      <td>4.974975</td>\n      <td>5.075905</td>\n    </tr>\n    <tr>\n      <th>0.200</th>\n      <td>6.612375</td>\n      <td>6.016148</td>\n      <td>5.612526</td>\n      <td>5.342773</td>\n      <td>5.160531</td>\n      <td>4.815270</td>\n      <td>4.773145</td>\n      <td>4.913540</td>\n      <td>5.075985</td>\n      <td>5.126820</td>\n      <td>5.165297</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Learning rate and iterations count for linear regression\n",
    "alphas, steps = [0.001, 0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.07, 0.1, 0.2], [20, 40, 60, 80, 100, 200, 400, 800, 1500, 2000, 3000]\n",
    "# Test linear regression\n",
    "results = test(LinearRegression, alphas, steps, X_norm, Y_raw)\n",
    "# Show results\n",
    "pd.DataFrame(results, index=alphas, columns=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         1         2         3         4         5         8         10  \\\n",
       "E  6.475398  6.082422  5.721199  5.636515  5.620032  5.563837  5.613480   \n",
       "G  6.679708  6.812917  6.990575  7.063891  7.111383  7.253345  7.306131   \n",
       "P  5.818329  5.621769  5.639056  5.631617  5.711742  5.865409  5.908960   \n",
       "T  6.475398  6.084789  5.730255  5.635753  5.614309  5.535383  5.574117   \n",
       "Q  6.475428  6.193432  5.962292  5.841294  5.796370  5.624368  5.597792   \n",
       "S  7.260799  7.350942  7.512244  7.557822  7.589478  7.667073  7.696137   \n",
       "\n",
       "         12        15        17        20        25  \n",
       "E  5.675565  5.726785  5.741245  5.771925  5.845559  \n",
       "G  7.348097  7.414750  7.441102  7.476916  7.527136  \n",
       "P  5.923147  5.915575  5.953784  6.005255  6.013253  \n",
       "T  5.628422  5.673372  5.686308  5.717320  5.788801  \n",
       "Q  5.586039  5.589468  5.583391  5.605161  5.657234  \n",
       "S  7.721997  7.762200  7.777821  7.802788  7.848259  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>8</th>\n      <th>10</th>\n      <th>12</th>\n      <th>15</th>\n      <th>17</th>\n      <th>20</th>\n      <th>25</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>E</th>\n      <td>6.475398</td>\n      <td>6.082422</td>\n      <td>5.721199</td>\n      <td>5.636515</td>\n      <td>5.620032</td>\n      <td>5.563837</td>\n      <td>5.613480</td>\n      <td>5.675565</td>\n      <td>5.726785</td>\n      <td>5.741245</td>\n      <td>5.771925</td>\n      <td>5.845559</td>\n    </tr>\n    <tr>\n      <th>G</th>\n      <td>6.679708</td>\n      <td>6.812917</td>\n      <td>6.990575</td>\n      <td>7.063891</td>\n      <td>7.111383</td>\n      <td>7.253345</td>\n      <td>7.306131</td>\n      <td>7.348097</td>\n      <td>7.414750</td>\n      <td>7.441102</td>\n      <td>7.476916</td>\n      <td>7.527136</td>\n    </tr>\n    <tr>\n      <th>P</th>\n      <td>5.818329</td>\n      <td>5.621769</td>\n      <td>5.639056</td>\n      <td>5.631617</td>\n      <td>5.711742</td>\n      <td>5.865409</td>\n      <td>5.908960</td>\n      <td>5.923147</td>\n      <td>5.915575</td>\n      <td>5.953784</td>\n      <td>6.005255</td>\n      <td>6.013253</td>\n    </tr>\n    <tr>\n      <th>T</th>\n      <td>6.475398</td>\n      <td>6.084789</td>\n      <td>5.730255</td>\n      <td>5.635753</td>\n      <td>5.614309</td>\n      <td>5.535383</td>\n      <td>5.574117</td>\n      <td>5.628422</td>\n      <td>5.673372</td>\n      <td>5.686308</td>\n      <td>5.717320</td>\n      <td>5.788801</td>\n    </tr>\n    <tr>\n      <th>Q</th>\n      <td>6.475428</td>\n      <td>6.193432</td>\n      <td>5.962292</td>\n      <td>5.841294</td>\n      <td>5.796370</td>\n      <td>5.624368</td>\n      <td>5.597792</td>\n      <td>5.586039</td>\n      <td>5.589468</td>\n      <td>5.583391</td>\n      <td>5.605161</td>\n      <td>5.657234</td>\n    </tr>\n    <tr>\n      <th>S</th>\n      <td>7.260799</td>\n      <td>7.350942</td>\n      <td>7.512244</td>\n      <td>7.557822</td>\n      <td>7.589478</td>\n      <td>7.667073</td>\n      <td>7.696137</td>\n      <td>7.721997</td>\n      <td>7.762200</td>\n      <td>7.777821</td>\n      <td>7.802788</td>\n      <td>7.848259</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Kernels and neighbors count (dynamic parzen window usage) for Nadaraya-Watson method\n",
    "kernels, neighbors = [E, G, P, T, Q, S], [1, 2, 3, 4, 5, 8, 10, 12, 15, 17, 20, 25]\n",
    "# Test Nadaraya-Watson method\n",
    "results = test(NadarayaWatson, kernels, neighbors, X_norm, Y_raw)\n",
    "# Show results\n",
    "pd.DataFrame(results, index=[k.__name__ for k in kernels], columns=neighbors)"
   ]
  },
  {
   "source": [
    "Further in Russian :)\n",
    "\n",
    "Как я и ожидал при увеличении количества итераций точность вычисления для линейной регрессии улучшается (уменьшается RMSE). Повышенные значения в конце (на 3000 итерациях) означают достижения локального минимума, а значит на дальнейших итерациях алгоритм будет \"прыгать\" в окрестности локального минимума. Наилучшие показатели достигаются при значениях коэффициента скорости обучения 0.03, 0.04, 0.05. При бОльших скоростях обучения мы быстрее достигаем локального минимума (что логично), но тем самым алгоритму сложнее его достигнут (собственно из большого коэффициента скорости обучения).  \n",
    "\n",
    "Метод Надаря-Ватсона показал чуть похуже результаты (чего я не ожидал), но близкие к параметрам линейной регресии для скорости обучения 0.001, 0.002, 0.2. Однако я ожидал, что изменения ядер не будет сильно влиять на результаты регрессии (были использованы как инфинитные, так и не инфинитные ядра). Здесь стоит отметить, что инфинитные ядра оказываются хуже. Предполагаю, что это связано с тем, что \"хвосты\" таких ядер оказывают более существенное влияние на этом наборе данных, чем ядра, ограниченные носителем в виде индикатора.  \n",
    "\n",
    "Мне интересно стало понять почему казалось бы простая линейная регрессия (хотя для данного учебного набора это нормальная ситуация) показывает хоть немного, но лучшие результаты. Для начала стоит отменить, что методы, основанные на ядрах чувствительны к данным. В таблице с результатами метода Надарая-Ватсона мы можем заметить, что начиная с некоторого большого количества соседей результаты ухудшаются (при большем количестве соседей мы увеличиваем размер парзеновского окна, таким образом увеличивая влияние большего числа объектов), как и на семинаре (07) мы видели небольшое переобучение на большом количевстве соседей.  \n",
    "\n",
    "Поиследовав научную литературу, я нашёл подтверждение своим словам (например, [тут](http://www.ccas.ru/voron/download/Regression.pdf)):  \n",
    "1. Метод Надарая-Ватсона чувстителем к выбросам в данных.  \n",
    "2. При больших (и не только) размернотях (у нас размерност вектора данных = 13) возникают краевые эффекты, свзязаные со смещением аппроксимирующей функции относительно истинной зависмости из-за расположения объектов выборки не вокруг заданного объекта, а по одну сторону от него.\n",
    "\n",
    "В добавок ко всему выше сказанному скажу, что исходный датасет не является большим (всего 506 векторов), что, возможно, также может делать его приятным для линейной регресии.\n",
    "\n",
    "## Заключение\n",
    "\n",
    "В данной работе реализованы методы линейной регрессии и регрессии методом Надаря-Ватсона. Было произведено тестирование этих методов на наборе Boston Housing Data Set и оценены результаты работы данных алгоритмов. Заключения содержат как собственную оценку результатов, так и изучение научной литературы. Процесс реализации данных алгоритмов и их непосредственное тестирование помогли лучше разобраться во внутреннем строением и особенностями предложенных методов.\n",
    "\n",
    "Спасибо за домашнее задание :)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}